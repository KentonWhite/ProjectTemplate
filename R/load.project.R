load.project <- function()
{
  cat('Loading project configuration\n')
  library('yaml')
  assign('config',
         yaml.load_file(file.path('config', 'global.yaml')),
         envir = .GlobalEnv)

  cat('Autoloading utility functions\n')
  source('lib/utilities.R')

  if (config[['load_libraries']] == 'on')
  {
    cat('Autoloading libraries\n')
    for (library.to.load in config[['libraries']])
    {
      cat(paste(' Loading library:', library.to.load, '\n'))
      library(library.to.load, character.only = TRUE)
    }
  }

  cat('Autoloading data\n')
  # Supported file extensions:
  #
  # .csv: CSV files that use a comma separator.
  # .csv.bz2: CSV files that use a comma separator and are compressed using bzip2.
  # .csv.zip: CSV files that use a comma separator and are compressed using zip.
  # .csv.gz: CSV files that use a comma separator and are compressed using gzip.
  # .tsv: CSV files that use a tab separator.
  # .tsv.bz2: CSV files that use a tab separator and are compressed using bzip2.
  # .tsv.zip: CSV files that use a tab separator and are compressed using zip.
  # .tsv.gz: CSV files that use a tab separator and are compressed using gzip.
  # .wsv: CSV files that use an arbitrary whitespace separator.
  # .wsv.bz2: CSV files that use an arbitrary whitespace separator and are compressed using bzip2.
  # .wsv.zip: CSV files that use an arbitrary whitespace separator and are compressed using zip.
  # .wsv.gz: CSV files that use an arbitrary whitespace separator and are compressed using gzip.
  # .RData: .RData binary files produced by save().
  # .rda: .RData binary files produced by save().
  # .url: A YAML file that contains an HTTP URL for a remote dataset.
  # .sql: A YAML file that contains database connection information for a MySQL database.
  # .sav: Binary file format generated by SPSS.
  # .dta: Binary file format generated by Stata.

  # Current data reader functions.
  CSVReader <- function(data.file, filename, variable.name)
  {
    assign(variable.name,
           read.csv(filename,
                    header = TRUE,
                    sep = ','),
           envir = .GlobalEnv)
  }

  TSVReader <- function(data.file, filename, variable.name)
  {
    assign(variable.name,
           read.csv(filename,
                    header = TRUE,
                    sep = '\t'),
           envir = .GlobalEnv)
  }

  WSVReader <- function(data.file, filename, variable.name)
  {
    assign(variable.name,
           read.csv(filename,
                    header = TRUE,
                    sep = ' '),
           envir = .GlobalEnv)
  }

  RDataReader <- function(data.file, filename, variable.name)
  {
    load(filename, envir = .GlobalEnv)
  }

  URLReader <- function(data.file, filename, variable.name)
  {
    # A .url file contains YAML describing the data source. 
	# Only one data source per file is supported.
    # An example file is shown below.
    #
    # url: "http://www.johnmyleswhite.com/ProjectTemplate/sample_data.csv"

    url.info <- yaml.load_file(filename)
	file.type <-""
	
    for (extension in names(extensions.dispatch.table))
      {
    	if(grepl(extension, url.info[['url']], ignore.case=TRUE, perl=TRUE))
			file.type <- extension
	}
	
	if(file.type=="") 
	  {
		print(paste("The source at", url.info[['url']], "was not processed properly."))
	} else 
	  {
		if(file.type %in% c("\\.Rdata$", "\\.Rda$")) 
		  {
			con <- url(url.info[['url']])
			RDataReader(data.file, con, variable.name)

		} else if(file.type %in% c("\\.xlsx$", "\\.sql$", "\\.sav%", "\\.dta$")) 
		  {
			tmp <- paste(tmp, Sys.time(), sep="")
			download.file(url.info[['url']], tmp)
			do.call(extensions.dispatch.table[[file.type]],
	                list(data.file,
	                     tmp,
	                     variable.name))   
		} else
		  {
			do.call(extensions.dispatch.table[[file.type]],
	                list(data.file,
	                     url.info[['url']],
	                     variable.name))   
		}
	}
  }

  SQLReader <- function(data.file, filename, variable.name)
  {
    # A .sql file contains YAML describing the data source.
    # Two example files are shown below.
    #
    # type: mysql
    # user: sample_user
    # password: sample_password
    # host: localhost
    # dbname: sample_database
    # table: sample_table
    #
    # type: sqlite
    # dbname: /path/to/sample_database
    # table: sample_table

    database.info <- yaml.load_file(filename)

    if (! (database.info[['type']] %in% c('mysql', 'sqlite')))
    {
      warning('Only databases reachable through RMySQL and RSQLite
               are currently supported.')
      assign(variable.name,
             NULL,
             envir = .GlobalEnv)
      return()
    }

    if (database.info[['type']] == 'mysql')
    {
      library('RMySQL')
      mysql.driver <- dbDriver("MySQL")

      connection <- dbConnect(mysql.driver,
                              user = database.info[['user']],
                              password = database.info[['password']],
                              host = database.info[['host']],
                              dbname = database.info[['dbname']])
    }

    if (database.info[['type']] == 'sqlite')
    {
      library('RSQLite')
      sqlite.driver <- dbDriver("SQLite")

      connection <- dbConnect(sqlite.driver,
                              dbname = database.info[['dbname']])
    }

    if (dbExistsTable(connection, database.info[['table']]))
    {
      data.parcel <- dbReadTable(connection,
                                 database.info[['table']],
                                 row.names = NULL)

      assign(variable.name,
             data.parcel,
             envir = .GlobalEnv)
    }
    else
    {
      warning(paste('Table not found:', database.info[['table']]))
      return()
    }

    # If the table exists but is empty, do not create a variable.
    if (nrow(data.parcel) == 0)
    {
      assign(variable.name,
             NULL,
             envir = .GlobalEnv)
      return()
    }

    # Disconnect from database resources. Warn if failure.
    disconnect.success <- dbDisconnect(connection)

    if (! disconnect.success)
    {
      warning(paste('Unable to disconnect from database:',
                    database.info[['dbname']]))
    }
  }

  SPSSReader <- function(data.file, filename, variable.name)
  {
    library('foreign')

    assign(variable.name,
           read.spss(filename),
           envir = .GlobalEnv)
  }

  StataReader <- function(data.file, filename, variable.name)
  {
    library('foreign')

    assign(variable.name,
           read.dta(filename),
           envir = .GlobalEnv)
  }

  XLSReader <- function(data.file, filename, workbook.name)
  {
    library('gdata')
    
    sheets <- sheetNames(filename)

    for (sheet.name in sheets)
    {
      variable.name <- paste(workbook.name, clean.variable.name(sheet.name), sep = ".")
     tryCatch(assign(variable.name,
             read.xls(filename,
                       sheet = sheet.name),
             envir = .GlobalEnv),
			error=function(e) print(paste("The worksheet", sheet.name, "didn't load correctly."))
	  )
    }
  }


  XLSXReader <- function(data.file, filename, workbook.name)
  {
    library('xlsx')
    
    wb <- loadWorkbook(filename)
    sheets <- getSheets(wb)

    for (sheet.name in names(sheets))
    {
      variable.name <- paste(workbook.name, clean.variable.name(sheet.name), sep = ".")
     tryCatch(assign(variable.name,
             read.xlsx(filename,
                       sheetName = sheet.name,
                       header = TRUE),
             envir = .GlobalEnv),
			error=function(e) print(paste("The worksheet", sheet.name, "didn't load correctly."))
		)
    }
  }

  # Use a list to map file extension detection regular expressions to the
  # appropriate reader functions.
  extensions.dispatch.table <- list("\\.csv$" = CSVReader,
                                    "\\.csv.bz2$" = CSVReader,
                                    "\\.csv.zip$" = CSVReader,
                                    "\\.csv.gz$" = CSVReader,
                                    "\\.tsv$" = TSVReader,
                                    "\\.tsv.bz2$" = TSVReader,
                                    "\\.tsv.zip$" = TSVReader,
                                    "\\.tsv.gz$" = TSVReader,
                                    "\\.wsv$" = WSVReader,
                                    "\\.wsv.bz2$" = WSVReader,
                                    "\\.wsv.zip$" = WSVReader,
                                    "\\.wsv.gz$" = WSVReader,
                                    "\\.Rdata$" = RDataReader,
                                    "\\.rda$" = RDataReader,
                                    "\\.url$" = URLReader,
                                    "\\.sql$" = SQLReader,
                                    "\\.xls$" = XLSReader,
                                    "\\.xlsx$" = XLSXReader, 
									"\\.sav$" = SPSSReader,
									"\\.dta$" = StataReader)

  # First, we load everything out of cache/.
  cache.files <- dir('cache')

  for (cache.file in cache.files)
  {
    for (extension in names(extensions.dispatch.table))
    {
      filename <- file.path('cache', cache.file)

      if (grepl(extension, cache.file, ignore.case = TRUE, perl = TRUE))
      {
        variable.name <- clean.variable.name(sub(extension,
                                                 '',
                                                 cache.file,
                                                 ignore.case = TRUE,
                                                 perl = TRUE))

        cat(paste(" Loading cached data set: ", variable.name, '\n', sep = ''))

        do.call(extensions.dispatch.table[[extension]],
                list(cache.file,
                     filename,
                     variable.name))

        break()
      }
    }
  }

  # Then we consider loading things from data/.
  if (config[['data_loading']] == 'on')
  {
    data.files <- dir('data')

    for (data.file in data.files)
    {
      for (extension in names(extensions.dispatch.table))
      {
        filename <- file.path('data', data.file)

        if (grepl(extension, data.file, ignore.case = TRUE, perl = TRUE))
        {
          variable.name <- clean.variable.name(sub(extension,
                                                   '',
                                                   data.file,
                                                   ignore.case = TRUE,
                                                   perl = TRUE))

          # If this variable already exists in cache, don't load it from data.
          if (variable.name %in% ls(envir = .GlobalEnv))
          {
            next()
          }

          cat(paste(" Loading data set: ", variable.name, '\n', sep = ''))

          do.call(extensions.dispatch.table[[extension]],
                  list(data.file,
                       filename,
                       variable.name))

          break()
        }
      }
    }
  }

  if (config[['munging']] == 'on')
  {
    cat('Munging data\n')
    for (preprocessing.script in sort(dir('munge')))
    {
      cat(paste(' Running preprocessing script:', preprocessing.script, '\n'))
      source(file.path('munge', preprocessing.script))
    }
  }

  if (config[['logging']] == 'on')
  {
    cat('Initializing logger\n')
    library('log4r')
    # Need to think about why this didn't work in the naive way.
    # Something about how `level<-` works.
    logger <- create.logger()
    logfile(logger) <- file.path('logs', 'project.log')
    level(logger) <- log4r:::INFO
    assign('logger', logger, envir = .GlobalEnv)
  }
}
